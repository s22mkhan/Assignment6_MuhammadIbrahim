{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95be233-dd2f-4998-803a-71564225ca37",
   "metadata": {},
   "source": [
    "# Assignment 6: Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf52164-6378-4105-886f-3ee9c7fe21b2",
   "metadata": {},
   "source": [
    "Submitted by: **Muhammad Ibrahim Afsar Khan**\n",
    "\n",
    "\n",
    "Task 2:\n",
    "- Implement a fully convolutional DCGAN-like model (https://arxiv.org/abs/1511.06434)\n",
    "- Train the model on the CelebA dataset to generate new faces\n",
    "- Requirements:\n",
    "    - Use Tensorboard, WandDB or some other experiment tracker\n",
    "    - Show the capabilities of your model to generate images\n",
    "    - Evaluate and track during training using one quantitative metric (e.g. FID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f137d8-e377-4714-99f0-01a8acd10e8a",
   "metadata": {},
   "source": [
    "## Prelimenaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ec39c71-60d6-483b-8de8-4426f2c513ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af5863d9-c26e-4121-a951-7d327b2b92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec080e2e-2ff6-454d-96c6-0643e5231be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TBOARD_LOGS = os.path.join(os.getcwd(), \"tboard_logs\", \"fully_convolutional_DCGAN\")\n",
    "if not os.path.exists(TBOARD_LOGS):\n",
    "    os.makedirs(TBOARD_LOGS)\n",
    "\n",
    "shutil.rmtree(TBOARD_LOGS)\n",
    "writer = SummaryWriter(TBOARD_LOGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69d941-3660-4933-8f6e-12b714908fe4",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63591482-ee9b-41ef-b25e-db5f19945848",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './img_align_celeba/img_align_celeba'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d972b164-74db-4a9d-ab9e-3e5d1fe5f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith('.jpg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # print(f\"Image shape (with batch dim): {image.shape}\")  # Debug print\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "693de24d-0df4-4981-9a6b-a62427563866",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = CelebADataset(root_dir=dataset_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6874061e-6b82-4c74-ad1c-2d97daf733f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787db82-3b8e-42a8-9207-a1540e5c5970",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec864d-9c2d-45d2-8b10-08093b95b52d",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02a62e7d-d36f-4b3b-ac06-7c52977197ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully convolutional generator using ReLU activations. \n",
    "    Takes as input a latent vector and outputs a fake sample.\n",
    "       (B, latent_dim, 1, 1)  --> (B, num_channels, 64, 64)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128, num_channels=3, base_channels=64):\n",
    "        \"\"\" Model initializer \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(4):\n",
    "            layers.append(\n",
    "                ConvTransposeBlock(\n",
    "                        in_channels=latent_dim if i == 0 else base_channels * 2 ** (3-i+1),\n",
    "                        out_channels=base_channels * 2 ** (3-i),\n",
    "                        kernel_size=4,\n",
    "                        stride=1 if i == 0 else 2,\n",
    "                        add_norm=True,\n",
    "                        activation=\"ReLU\"\n",
    "                    )\n",
    "                )\n",
    "        layers.append(\n",
    "            ConvTransposeBlock(\n",
    "                    in_channels=base_channels,\n",
    "                    out_channels=num_channels,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    add_norm=False,\n",
    "                    activation=\"Tanh\"\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through generator \"\"\"\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd4583-faea-4539-9877-8e4558feb6ab",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3650d9ea-b958-4af0-b922-621c4f46d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" A fully convolutional discriminator using LeakyReLU activations. \n",
    "    Takes as input either a real or fake sample and predicts its authenticity.\n",
    "       (B, num_channels, 64, 64)  -->  (B, 1, 1, 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_dim=1, base_channels=64, dropout=0.3):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        super().__init__()  \n",
    "        \n",
    "        layers = []\n",
    "        for i in range(4):\n",
    "            layers.append(\n",
    "                ConvBlock(\n",
    "                        in_channels=in_channels if i == 0 else base_channels * 2 ** i,\n",
    "                        out_channels=base_channels * 2 ** (i + 1),\n",
    "                        kernel_size=4,\n",
    "                        add_norm=True,\n",
    "                        activation=\"LeakyReLU\",\n",
    "                        dropout=dropout,\n",
    "                        stride=2\n",
    "                    )\n",
    "                )\n",
    "        layers.append(\n",
    "                ConvBlock(\n",
    "                        in_channels=base_channels * 16,\n",
    "                        out_channels=out_dim,\n",
    "                        kernel_size=4,\n",
    "                        stride=4,\n",
    "                        add_norm=False,\n",
    "                        activation=\"Sigmoid\"\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        return\n",
    "      \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35d210-3f0e-41eb-a753-6120b2061723",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f579143-18a0-4c32-83e7-e8d32a0a9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93fe739a-965d-44ea-810e-a235c0167290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): ConvTransposeBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ConvTranspose2d(128, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): ConvTransposeBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): ConvTransposeBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): ConvTransposeBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): ConvTransposeBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3b088-0655-4854-ace3-5556af95aca4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5629f51-9e48-48e2-8136-727732172f58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Measure discriminator's ability to classify real from generated samples\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m fake_loss \u001b[38;5;241m=\u001b[39m criterion(discriminator(gen_imgs\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), fake)\n\u001b[1;32m     75\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (real_loss \u001b[38;5;241m+\u001b[39m fake_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/CudaLab/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/CudaLab/lib/python3.8/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CudaLab/lib/python3.8/site-packages/torch/nn/functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3087\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3089\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3092\u001b[0m     )\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3095\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming CelebADataset and DataLoader are already defined as provided\n",
    "# Assuming Generator and Discriminator classes are defined as previously provided\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 50\n",
    "latent_dim = 128\n",
    "sample_interval = 500  # Interval to save generated images\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim=latent_dim, num_channels=3).to(device)\n",
    "discriminator = Discriminator(in_channels=3).to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Directory to save generated images\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, imgs in enumerate(data_loader):\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((imgs.size(0), 1), requires_grad=False).to(device)\n",
    "        fake = torch.zeros((imgs.size(0), 1), requires_grad=False).to(device)\n",
    "        \n",
    "        # Configure input\n",
    "        real_imgs = imgs.to(device)\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn((imgs.size(0), latent_dim, 1, 1)).to(device)\n",
    "        \n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = criterion(discriminator(gen_imgs).view(-1, 1), valid)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = criterion(discriminator(real_imgs).view(-1, 1), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()).view(-1, 1), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % sample_interval == 0:\n",
    "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(data_loader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "            \n",
    "            # Save some generated images\n",
    "            save_image(gen_imgs.data[:25], f\"generated_images/{epoch}_{i}.png\", nrow=5, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7a450-6497-45b1-a4d5-af90c7287e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf055a0-7b90-473e-ad8f-aacda269df16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
