{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95be233-dd2f-4998-803a-71564225ca37",
   "metadata": {},
   "source": [
    "# Assignment 6: Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf52164-6378-4105-886f-3ee9c7fe21b2",
   "metadata": {},
   "source": [
    "Submitted by: **Muhammad Ibrhaim Afsar Khan**\n",
    "\n",
    "\n",
    "Task 1:\n",
    "\n",
    "- Implement a conditional DCGAN model (https://arxiv.org/abs/1411.1784)\n",
    "- Train the model for conditional generation on the SVHN dataset\n",
    "- Requirements:\n",
    "    - Use Tensorboard, WandDB or some other experiment tracker\n",
    "    - Show the capabilities of the model to generate data based on given label\n",
    "\n",
    "Task 2:\n",
    "- Implement a fully convolutional DCGAN-like model (https://arxiv.org/abs/1511.06434)\n",
    "- Train the model on the CelebA dataset to generate new faces\n",
    "- Requirements:\n",
    "    - Use Tensorboard, WandDB or some other experiment tracker\n",
    "    - Show the capabilities of your model to generate images\n",
    "    - Evaluate and track during training using one quantitative metric (e.g. FID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f137d8-e377-4714-99f0-01a8acd10e8a",
   "metadata": {},
   "source": [
    "## Prelimenaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ec39c71-60d6-483b-8de8-4426f2c513ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af5863d9-c26e-4121-a951-7d327b2b92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec080e2e-2ff6-454d-96c6-0643e5231be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TBOARD_LOGS = os.path.join(os.getcwd(), \"tboard_logs\", \"conditional_DCGAN\")\n",
    "if not os.path.exists(TBOARD_LOGS):\n",
    "    os.makedirs(TBOARD_LOGS)\n",
    "\n",
    "shutil.rmtree(TBOARD_LOGS)\n",
    "writer = SummaryWriter(TBOARD_LOGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69d941-3660-4933-8f6e-12b714908fe4",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "693de24d-0df4-4981-9a6b-a62427563866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_dataset = datasets.SVHN(root='../data', split='train', download=True, transform=transform)\n",
    "test_dataset = datasets.SVHN(root='../data', split='test', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6874061e-6b82-4c74-ad1c-2d97daf733f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787db82-3b8e-42a8-9207-a1540e5c5970",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8f39789-1b93-4ff2-b295-04e0121ff8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "LATENT_DIM = 100\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 3e-4\n",
    "BETA1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec864d-9c2d-45d2-8b10-08093b95b52d",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02a62e7d-d36f-4b3b-ac06-7c52977197ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(NUM_CLASSES, NUM_CLASSES)\n",
    "\n",
    "        self.init_size = IMAGE_SIZE // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(LATENT_DIM + NUM_CLASSES, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd4583-faea-4539-9877-8e4558feb6ab",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3650d9ea-b958-4af0-b922-621c4f46d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(NUM_CLASSES, NUM_CLASSES)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3 + 1, 64, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(512, 1, 3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        labels = self.label_emb(labels).view(labels.shape[0], 1, 32, 32)\n",
    "        d_in = torch.cat((img, labels), 1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity.view(validity.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35d210-3f0e-41eb-a753-6120b2061723",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16936d36-73f9-441a-af48-53841e2ff080",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[256, 1, 32, 32]' is invalid for input of size 2560",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m generator(z, labels)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Loss measures generator's ability to fool the discriminator\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m, valid)\n\u001b[1;32m     42\u001b[0m g_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     43\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/CudaLab/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[33], line 27\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, img, labels)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, labels):\n\u001b[0;32m---> 27\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     d_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((img, labels), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m     validity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(d_in)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[256, 1, 32, 32]' is invalid for input of size 2560"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9))\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(batch_size, 1, requires_grad=False).to(device)\n",
    "        fake = torch.zeros(batch_size, 1, requires_grad=False).to(device)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(batch_size, LATENT_DIM).to(device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs, labels), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs, labels), valid)\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), labels), fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{EPOCHS}] [Batch {i}/{len(train_loader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "        # Logging to Tensorboard\n",
    "        writer.add_scalar('Loss/Discriminator', d_loss.item(), epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('Loss/Generator', g_loss.item(), epoch * len(train_loader) + i)\n",
    "\n",
    "    # Save generator's output on fixed noise\n",
    "    if epoch % 10 == 0:\n",
    "        fixed_noise = torch.randn(10, LATENT_DIM).to(device)\n",
    "        fixed_labels = torch.tensor([num for num in range(10)]).to(device)\n",
    "        with torch.no_grad():\n",
    "            fixed_gen_imgs = generator(fixed_noise, fixed_labels).cpu()\n",
    "        grid = torchvision.utils.make_grid(fixed_gen_imgs, nrow=5, normalize=True)\n",
    "        writer.add_image('Generated Images', grid, epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f579143-18a0-4c32-83e7-e8d32a0a9bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
